# -*- coding: utf-8 -*-
"""Introducao_nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/101eCAoFUDsUncvzgaKxUmMMbxIX8o7XJ

#Introdução a NLP
Processamento de Linguagem Natural
"""

import keras
import nltk
import pandas as pd
import numpy as np
import re
import codecs
import itertools
import matplotlib
import matplotlib.patches as mpatches
import matplotlib.pyplot as plt
import numpy as np
import gensim

# Inspecionando os dados 
url = 'https://raw.githubusercontent.com/hundredblocks/concrete_NLP_tutorial/master/socialmedia_relevant_cols.csv'
questions = pd.read_csv(url, encoding='ISO-8859-1')
questions.columns=['text', 'choose_one', 'class_label']

"""###Dados sujos:

"""

#Primeiras linhas 
questions.head()

"""###Tabela 1

"""

#Últimas linhas
questions.tail()

"""###Estatística descritiva:"""

questions.describe()

"""###Método de limpeza:"""

# Expressões regulares para limpeza dos dados 
def standardize_text(df, text_field):    
    df[text_field] = df[text_field].str.replace(r"http\S+", "")    
    df[text_field] = df[text_field].str.replace(r"http", "")    
    df[text_field] = df[text_field].str.replace(r"@\S+", "")    
    df[text_field] = df[text_field].str.replace(r"[^A-Za-z0-9(),!?@\'\`\"\_\n]", " ")    
    df[text_field] = df[text_field].str.replace(r"@", "at")    
    df[text_field] = df[text_field].str.lower()    
    return df

#Limpeza e regravação do arquivo de saída limpo 
clean_questions = standardize_text(questions, "text") 
clean_questions.to_csv("clean_data.csv")

"""###Dados limpos:

"""

#Primeiras linhas 
clean_questions.head()

#Últimas linhas 
clean_questions.tail()

"""####Podemos observar que além das letras serem transformadas em minúsculo, também foram removidos caracteres especiais entre outras coisas.

###Distribuição das classes:
"""

clean_questions.groupby("class_label").count()

"""###Quebrando os dados
Agora que temos os dados limpos, vamos transformá-los para que o modelo possa entender. Logo:

-Quebrar as sentenças em listas de palavras separadas;

-Dividir os dados para treinamento e teste do modelo;

-Inspecionar os dados novamente.
"""

from nltk.tokenize import RegexpTokenizer 
# Método de quebra dos dados 
tokenizer = RegexpTokenizer(r'\w+') 
# Gerando listas de sentenças quebradas 
clean_questions["tokens"] = clean_questions["text"].apply(tokenizer.tokenize)

#Primeiras linhas 
clean_questions.head()

#Últimas linhas 
clean_questions.tail()

#Inspecioanndo novamente os dados 
all_words = [word for tokens in clean_questions["tokens"] for word in tokens] 
sentence_lengths = [len(tokens) for tokens in clean_questions["tokens"]] 
VOCAB = sorted(list(set(all_words))) 
print("%s Quantidade total de palavras, com um vocabulario de %s" % (len(all_words), len(VOCAB))) 
print("Tamanho máximo de uma sentença %s" % max(sentence_lengths))

#Distribuilção das sentenças por quantidade de palavras 
fig = plt.figure(figsize=(10, 10)) 
plt.xlabel('Tamanho da setença') 
plt.ylabel('Número de sentenças') 
plt.hist(sentence_lengths) 
plt.show()